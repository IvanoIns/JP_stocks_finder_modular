"""
Universe A/B scorecard for burst-capture evaluation.

Compares baseline vs shadow arm using files generated by burst_audit.py:
- results/burst_audit/ab_audit_master.csv
- results/daily_picks/daily_picks_YYYY-MM-DD.csv
- results/daily_picks_ab/daily_picks_shadow_YYYY-MM-DD.csv
"""

from __future__ import annotations

import argparse
import json
from datetime import datetime
from pathlib import Path

import pandas as pd


DEFAULT_AB_MASTER = Path("results/burst_audit/ab_audit_master.csv")
DEFAULT_BASELINE_PICKS_DIR = Path("results/daily_picks")
DEFAULT_SHADOW_PICKS_DIR = Path("results/daily_picks_ab")
DEFAULT_DAILY_OUT = Path("results/burst_audit/ab_scorecard_daily.csv")
DEFAULT_SUMMARY_OUT = Path("results/burst_audit/ab_scorecard_latest.json")


REQUIRED_AB_COLUMNS = [
    "burst_date",
    "signal_date",
    "symbol",
    "captured_a_topn",
    "captured_b_topn",
]


def _to_bool(series: pd.Series) -> pd.Series:
    if series.dtype == bool:
        return series
    return series.astype(str).str.lower().isin({"1", "true", "t", "yes", "y"})


def _safe_div(numerator: float, denominator: float) -> float | None:
    if denominator == 0:
        return None
    return float(numerator) / float(denominator)


def _count_top_n_rows(csv_path: Path, top_n: int) -> int:
    if not csv_path.exists():
        return 0
    try:
        df = pd.read_csv(csv_path)
    except Exception:
        return 0
    if df.empty:
        return 0
    return int(min(top_n, len(df)))


def _build_daily_stats(
    ab_df: pd.DataFrame,
    *,
    top_n: int,
    baseline_picks_dir: Path,
    shadow_picks_dir: Path,
) -> pd.DataFrame:
    rows: list[dict] = []
    grouped = ab_df.groupby("burst_date", sort=True)

    for burst_date, day in grouped:
        signal_date = str(day["signal_date"].iloc[0])

        captured_a = _to_bool(day["captured_a_topn"]).sum()
        captured_b = _to_bool(day["captured_b_topn"]).sum()
        total_bursts = len(day)

        captured_a_s = _to_bool(day["captured_a_topn"])
        captured_b_s = _to_bool(day["captured_b_topn"])
        new_hits = int((captured_b_s & ~captured_a_s).sum())
        lost_hits = int((captured_a_s & ~captured_b_s).sum())

        baseline_picks_path = baseline_picks_dir / f"daily_picks_{signal_date}.csv"
        shadow_picks_path = shadow_picks_dir / f"daily_picks_shadow_{signal_date}.csv"
        picks_a_topn = _count_top_n_rows(baseline_picks_path, top_n)
        picks_b_topn = _count_top_n_rows(shadow_picks_path, top_n)

        capture_rate_a = _safe_div(captured_a, total_bursts)
        capture_rate_b = _safe_div(captured_b, total_bursts)
        precision_a = _safe_div(captured_a, picks_a_topn)
        precision_b = _safe_div(captured_b, picks_b_topn)

        rows.append(
            {
                "burst_date": burst_date,
                "signal_date": signal_date,
                "total_bursts": int(total_bursts),
                "captured_a_topn": int(captured_a),
                "captured_b_topn": int(captured_b),
                "capture_rate_a_topn": capture_rate_a,
                "capture_rate_b_topn": capture_rate_b,
                "picks_a_topn": int(picks_a_topn),
                "picks_b_topn": int(picks_b_topn),
                "precision_a_topn": precision_a,
                "precision_b_topn": precision_b,
                "new_hits_b_not_a": int(new_hits),
                "lost_hits_a_not_b": int(lost_hits),
                "delta_capture_b_minus_a": int(captured_b - captured_a),
            }
        )

    daily = pd.DataFrame(rows)
    if not daily.empty:
        daily = daily.sort_values("burst_date").reset_index(drop=True)
    return daily


def _relative_change(new_v: float | None, base_v: float | None) -> float | None:
    if base_v is None:
        if new_v is None:
            return None
        if new_v > 0:
            return float("inf")
        return 0.0
    if base_v == 0:
        if new_v is None or new_v == 0:
            return 0.0
        return float("inf")
    if new_v is None:
        return None
    return (new_v - base_v) / base_v


def _build_summary(
    daily: pd.DataFrame,
    *,
    window_days: int,
    min_days: int,
    min_rel_capture_improvement: float,
    max_rel_precision_drop: float,
    top_n: int,
    ab_master_path: Path,
) -> dict:
    window_df = daily.tail(window_days).copy() if not daily.empty else daily.copy()
    observed_days = len(window_df)

    bursts_total = int(window_df["total_bursts"].sum()) if observed_days else 0
    captures_a = int(window_df["captured_a_topn"].sum()) if observed_days else 0
    captures_b = int(window_df["captured_b_topn"].sum()) if observed_days else 0
    picks_a = int(window_df["picks_a_topn"].sum()) if observed_days else 0
    picks_b = int(window_df["picks_b_topn"].sum()) if observed_days else 0
    new_hits = int(window_df["new_hits_b_not_a"].sum()) if observed_days else 0
    lost_hits = int(window_df["lost_hits_a_not_b"].sum()) if observed_days else 0

    capture_rate_a = _safe_div(captures_a, bursts_total)
    capture_rate_b = _safe_div(captures_b, bursts_total)
    precision_a = _safe_div(captures_a, picks_a)
    precision_b = _safe_div(captures_b, picks_b)

    rel_capture_change = _relative_change(capture_rate_b, capture_rate_a)
    rel_precision_change = _relative_change(precision_b, precision_a)

    pass_capture = (
        rel_capture_change is not None
        and rel_capture_change != float("inf")
        and rel_capture_change >= min_rel_capture_improvement
    ) or (rel_capture_change == float("inf"))
    pass_precision = (
        rel_precision_change is not None
        and rel_precision_change >= (-1.0 * max_rel_precision_drop)
    )

    sufficient_data = observed_days >= min_days
    if not sufficient_data:
        verdict = "insufficient_data"
    elif pass_capture and pass_precision:
        verdict = "promote_shadow"
    else:
        verdict = "keep_baseline"

    return {
        "generated_at": datetime.now().isoformat(timespec="seconds"),
        "inputs": {
            "ab_master_path": str(ab_master_path),
            "top_n": int(top_n),
            "window_days": int(window_days),
            "min_days": int(min_days),
        },
        "thresholds": {
            "min_rel_capture_improvement": float(min_rel_capture_improvement),
            "max_rel_precision_drop": float(max_rel_precision_drop),
        },
        "window": {
            "observed_days": int(observed_days),
            "start_burst_date": (window_df["burst_date"].iloc[0] if observed_days else None),
            "end_burst_date": (window_df["burst_date"].iloc[-1] if observed_days else None),
        },
        "aggregates": {
            "total_bursts": bursts_total,
            "captures_a_topn": captures_a,
            "captures_b_topn": captures_b,
            "capture_rate_a_topn": capture_rate_a,
            "capture_rate_b_topn": capture_rate_b,
            "rel_capture_change_b_vs_a": rel_capture_change,
            "picks_a_topn": picks_a,
            "picks_b_topn": picks_b,
            "precision_a_topn": precision_a,
            "precision_b_topn": precision_b,
            "rel_precision_change_b_vs_a": rel_precision_change,
            "new_hits_b_not_a": new_hits,
            "lost_hits_a_not_b": lost_hits,
        },
        "decision": {
            "sufficient_data": bool(sufficient_data),
            "pass_capture_gate": bool(pass_capture),
            "pass_precision_gate": bool(pass_precision),
            "verdict": verdict,
        },
    }


def main() -> int:
    parser = argparse.ArgumentParser(description="Build rolling A/B scorecard from burst-audit outputs.")
    parser.add_argument("--ab-master", type=Path, default=DEFAULT_AB_MASTER, help="Path to ab_audit_master.csv")
    parser.add_argument("--top-n", type=int, default=10, help="Top-N picks compared in A/B")
    parser.add_argument("--window-days", type=int, default=20, help="Rolling burst-day window")
    parser.add_argument("--min-days", type=int, default=20, help="Minimum observed days for a decision")
    parser.add_argument(
        "--min-rel-capture-improvement",
        type=float,
        default=0.15,
        help="Minimum relative capture-rate improvement (B vs A) to pass",
    )
    parser.add_argument(
        "--max-rel-precision-drop",
        type=float,
        default=0.10,
        help="Maximum allowed relative precision drop (B vs A)",
    )
    parser.add_argument("--baseline-picks-dir", type=Path, default=DEFAULT_BASELINE_PICKS_DIR, help="Directory for baseline daily picks")
    parser.add_argument("--shadow-picks-dir", type=Path, default=DEFAULT_SHADOW_PICKS_DIR, help="Directory for shadow daily picks")
    parser.add_argument("--out-daily-csv", type=Path, default=DEFAULT_DAILY_OUT, help="Output path for daily scorecard CSV")
    parser.add_argument("--out-summary-json", type=Path, default=DEFAULT_SUMMARY_OUT, help="Output path for rolling summary JSON")
    args = parser.parse_args()

    if args.top_n <= 0:
        raise SystemExit("--top-n must be > 0")
    if args.window_days <= 0:
        raise SystemExit("--window-days must be > 0")
    if args.min_days <= 0:
        raise SystemExit("--min-days must be > 0")

    if not args.ab_master.exists():
        raise SystemExit(f"A/B master file not found: {args.ab_master}")

    ab_df = pd.read_csv(args.ab_master)
    if ab_df.empty:
        raise SystemExit(f"A/B master file is empty: {args.ab_master}")

    missing_cols = [c for c in REQUIRED_AB_COLUMNS if c not in ab_df.columns]
    if missing_cols:
        raise SystemExit(f"A/B master is missing required columns: {', '.join(missing_cols)}")

    ab_df["burst_date"] = ab_df["burst_date"].astype(str)
    ab_df["signal_date"] = ab_df["signal_date"].astype(str)

    daily = _build_daily_stats(
        ab_df,
        top_n=args.top_n,
        baseline_picks_dir=args.baseline_picks_dir,
        shadow_picks_dir=args.shadow_picks_dir,
    )
    if daily.empty:
        raise SystemExit("No daily A/B rows could be built.")

    summary = _build_summary(
        daily,
        window_days=args.window_days,
        min_days=args.min_days,
        min_rel_capture_improvement=args.min_rel_capture_improvement,
        max_rel_precision_drop=args.max_rel_precision_drop,
        top_n=args.top_n,
        ab_master_path=args.ab_master,
    )

    args.out_daily_csv.parent.mkdir(parents=True, exist_ok=True)
    args.out_summary_json.parent.mkdir(parents=True, exist_ok=True)
    daily.to_csv(args.out_daily_csv, index=False, encoding="utf-8")
    with open(args.out_summary_json, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    decision = summary["decision"]
    agg = summary["aggregates"]
    window = summary["window"]
    print("=" * 70)
    print("A/B SCORECARD")
    print("=" * 70)
    print(
        f"Window: {window['start_burst_date']} .. {window['end_burst_date']} "
        f"({window['observed_days']} day(s))"
    )
    print(f"Top-N: {args.top_n}")
    print(
        f"Capture A/B: {agg['captures_a_topn']}/{agg['total_bursts']} vs "
        f"{agg['captures_b_topn']}/{agg['total_bursts']}"
    )
    print(
        f"Precision A/B: {agg['precision_a_topn']} vs {agg['precision_b_topn']} "
        f"(picks {agg['picks_a_topn']} vs {agg['picks_b_topn']})"
    )
    print(f"Relative capture change (B vs A): {agg['rel_capture_change_b_vs_a']}")
    print(f"Relative precision change (B vs A): {agg['rel_precision_change_b_vs_a']}")
    print(f"Decision: {decision['verdict']}")
    print(f"Saved daily CSV: {args.out_daily_csv}")
    print(f"Saved summary JSON: {args.out_summary_json}")
    print("=" * 70)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
